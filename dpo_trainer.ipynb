{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9e0e7ba-c34f-457c-a8f2-f86122917934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.7.4: Fast Llama patching. Transformers: 4.53.2. vLLM: 0.9.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 4. Max memory: 23.691 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.7.4: Fast Llama patching. Transformers: 4.53.2. vLLM: 0.9.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 4. Max memory: 23.691 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build Direct Preference Optimization from scratch\n",
    "@author: demouo\n",
    "\"\"\"\n",
    "\n",
    "# Model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "\n",
    "model_path = \"Llama-3.2-1B-Instruct\"\n",
    "max_seq_length = 2056\n",
    "load_in_4bit = True\n",
    "# model = AutoModelForCausalLM(model_path, max_seq_length=max_seq_length)\n",
    "# tokenizer = AutoTokenizer(model_path)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_path,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=load_in_4bit\n",
    ")\n",
    "\n",
    "ref_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_path,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=load_in_4bit\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656248f2-9369-45a0-9165-fa8641c95dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT model\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "\n",
    "    r = 16,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 16,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0.1,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407, # Do not modify the random_state for reproducibility\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57682311-b845-4b34-9890-70c543e8361d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen_input_ids shape: torch.Size([1, 21])\n",
      "rejected_input_ids shape: torch.Size([1, 20])\n",
      "policy_chosen_log_probs shape: torch.Size([1, 21, 128256])\n",
      "policy_chosen_log_probs:\n",
      " tensor([[[-15.7500, -14.7500, -12.5625,  ..., -14.4375, -14.4375, -14.4375],\n",
      "         [-26.3750, -20.2500, -22.1250,  ..., -20.5000, -20.5000, -20.5000],\n",
      "         [-25.0000, -18.7500, -20.6250,  ..., -19.1250, -19.1250, -19.1250],\n",
      "         ...,\n",
      "         [-27.2500, -21.1250, -22.6250,  ..., -21.0000, -21.0000, -21.0000],\n",
      "         [-29.2500, -23.5000, -24.6250,  ..., -23.0000, -23.0000, -23.0000],\n",
      "         [-25.6250, -19.3750, -21.1250,  ..., -19.6250, -19.6250, -19.6250]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<LogSoftmaxBackward0>)\n",
      "policy_rejected_log_probs shape: torch.Size([1, 20, 128256])\n",
      "policy_rejected_log_probs:\n",
      " tensor([[[-15.8125, -14.9375, -12.6250,  ..., -14.4375, -14.4375, -14.4375],\n",
      "         [-26.1250, -20.1250, -21.7500,  ..., -20.2500, -20.2500, -20.2500],\n",
      "         [-26.0000, -19.7500, -21.6250,  ..., -19.7500, -19.7500, -19.7500],\n",
      "         ...,\n",
      "         [-27.1250, -21.0000, -22.6250,  ..., -21.0000, -21.0000, -21.0000],\n",
      "         [-25.0000, -18.8750, -20.5000,  ..., -18.8750, -18.8750, -18.8750],\n",
      "         [-25.2500, -19.1250, -20.7500,  ..., -19.3750, -19.3750, -19.3750]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<LogSoftmaxBackward0>)\n",
      "refer_chosen_log_probs shape: torch.Size([1, 21, 128256])\n",
      "refer_chosen_log_probs:\n",
      " tensor([[[-10.6875,  -9.6875,  -5.9375,  ..., -13.5000, -13.4375, -13.4375],\n",
      "         [-12.2500, -12.6250, -13.5625,  ..., -21.0000, -21.0000, -21.0000],\n",
      "         [-13.9375, -13.3750, -17.6250,  ..., -21.6250, -21.6250, -21.6250],\n",
      "         ...,\n",
      "         [ -7.0625, -12.5000, -17.2500,  ..., -22.2500, -22.2500, -22.2500],\n",
      "         [ -5.6562, -12.0625, -15.3750,  ..., -22.0000, -22.0000, -22.0000],\n",
      "         [ -8.0000, -12.3125, -11.5000,  ..., -21.1250, -21.1250, -21.1250]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "refer_rejected_log_probs shape: torch.Size([1, 20, 128256])\n",
      "refer_rejected_log_probs:\n",
      " tensor([[[-10.6875,  -9.6875,  -5.9375,  ..., -13.5000, -13.4375, -13.4375],\n",
      "         [-12.2500, -12.6250, -13.5625,  ..., -21.0000, -21.0000, -21.0000],\n",
      "         [-13.9375, -13.3750, -17.6250,  ..., -21.6250, -21.6250, -21.6250],\n",
      "         ...,\n",
      "         [ -7.2188, -10.5625, -13.4375,  ..., -20.6250, -20.6250, -20.6250],\n",
      "         [ -1.7500,  -8.6250, -11.0625,  ..., -18.6250, -18.6250, -18.6250],\n",
      "         [ -6.4375, -10.1250, -10.4375,  ..., -19.8750, -19.8750, -19.8750]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "prompt_length: 10\n",
      "policy_chosen_log_probs_response shape: torch.Size([1, 11, 128256])\n",
      "refer_chosen_log_probs_response shape: torch.Size([1, 11, 128256])\n",
      "policy_rejected_log_probs_response shape: torch.Size([1, 10, 128256])\n",
      "refer_rejected_log_probs_response shape: torch.Size([1, 10, 128256])\n",
      "labels_chosen shape: torch.Size([1, 11])\n",
      "labels_rejected shape: torch.Size([1, 10])\n",
      "labels shape: torch.Size([1, 11])\n",
      "After unsqueeze, labels shape: torch.Size([1, 11, 1])\n",
      "per_token_logps shape: torch.Size([1, 11, 1])\n",
      "response_log_probs shape: torch.Size([1])\n",
      "labels shape: torch.Size([1, 10])\n",
      "After unsqueeze, labels shape: torch.Size([1, 10, 1])\n",
      "per_token_logps shape: torch.Size([1, 10, 1])\n",
      "response_log_probs shape: torch.Size([1])\n",
      "pi_chosen_log_prob shape: torch.Size([1])\n",
      "pi_chosen_log_prob: tensor([-167.], device='cuda:0', dtype=torch.bfloat16, grad_fn=<SumBackward1>)\n",
      "pi_rejected_log_probs shape: torch.Size([1])\n",
      "pi_rejected_log_probs: tensor([-256.], device='cuda:0', dtype=torch.bfloat16, grad_fn=<SumBackward1>)\n",
      "labels shape: torch.Size([1, 11])\n",
      "After unsqueeze, labels shape: torch.Size([1, 11, 1])\n",
      "per_token_logps shape: torch.Size([1, 11, 1])\n",
      "response_log_probs shape: torch.Size([1])\n",
      "ref_chosen_log_prob shape: torch.Size([1])\n",
      "ref_chosen_log_prob: tensor([-43.], device='cuda:0', dtype=torch.bfloat16)\n",
      "labels shape: torch.Size([1, 10])\n",
      "After unsqueeze, labels shape: torch.Size([1, 10, 1])\n",
      "per_token_logps shape: torch.Size([1, 10, 1])\n",
      "response_log_probs shape: torch.Size([1])\n",
      "ref_rejected_log_prob shape: torch.Size([1])\n",
      "ref_rejected_log_prob: tensor([-80.5000], device='cuda:0', dtype=torch.bfloat16)\n",
      "policy_reward_chosen shape: torch.Size([1])\n",
      "policy_reward_rejected shape: torch.Size([1])\n",
      "loss shape: torch.Size([])\n",
      "loss: 0.0052490234375\n",
      "param shape: torch.Size([16, 2048])\n",
      "param: Parameter containing:\n",
      "tensor([[ 0.1017,  0.0170,  0.0056,  ...,  0.0788,  0.0931,  0.0857],\n",
      "        [ 0.0139,  0.0050, -0.1888,  ...,  0.0147,  0.2079,  0.1978],\n",
      "        [ 0.1138,  0.1156, -0.1026,  ...,  0.0087, -0.0024,  0.0801],\n",
      "        ...,\n",
      "        [-0.0208, -0.0977, -0.1069,  ...,  0.1901,  0.1030,  0.0781],\n",
      "        [ 0.1112, -0.0947, -0.0006,  ...,  0.1056, -0.0160, -0.1091],\n",
      "        [ 0.1940, -0.1057, -0.0834,  ...,  0.1127,  0.0925,  0.1102]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "param grad: tensor([[-2.4217e-06,  1.0544e-05,  2.4828e-05,  ..., -8.0718e-05,\n",
      "         -5.8365e-05, -1.5763e-05],\n",
      "        [-3.8980e-05, -8.6155e-05,  4.6209e-05,  ...,  4.3518e-05,\n",
      "         -5.2892e-05, -5.5045e-05],\n",
      "        [-3.6342e-05, -6.1730e-05,  2.9362e-05,  ...,  9.6931e-06,\n",
      "         -3.5444e-05, -1.1970e-05],\n",
      "        ...,\n",
      "        [ 2.9900e-05,  1.0939e-04, -1.4909e-04,  ..., -2.3249e-05,\n",
      "          4.3557e-05,  1.0684e-04],\n",
      "        [-6.9995e-06,  7.3152e-05, -1.1970e-04,  ..., -2.4633e-05,\n",
      "          2.4543e-07,  1.0960e-04],\n",
      "        [-2.6829e-06, -5.9290e-05,  1.6595e-04,  ..., -4.0931e-05,\n",
      "         -8.2916e-05, -1.5609e-04]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 164\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparam: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    163\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparam grad: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam.grad\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "model.train()\n",
    "ref_model.eval()\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-2)\n",
    "\n",
    "# One train example\n",
    "train_example = {\n",
    "    \"question\": \"I'm fat, how about you?\",\n",
    "    \"chosen\": \"Oh sorry to hear, what's the point now?\",\n",
    "    \"rejected\": \"Aha, so fat so funny your guy?\",\n",
    "}\n",
    "beta = 0.1\n",
    "prompt = train_example[\"question\"]\n",
    "chosen_response = train_example[\"chosen\"]\n",
    "rejected_response = train_example[\"rejected\"]\n",
    "\n",
    "chosen_input_text = prompt + tokenizer.eos_token + chosen_response\n",
    "chosen_tokenized_text = tokenizer(chosen_input_text, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "chosen_input_ids = chosen_tokenized_text.input_ids\n",
    "print(f\"chosen_input_ids shape: {chosen_input_ids.shape}\")\n",
    "chosen_attention_mask = chosen_tokenized_text.attention_mask\n",
    "\n",
    "rejected_input_text = prompt + tokenizer.eos_token + rejected_response\n",
    "rejected_tokenized_text = tokenizer(rejected_input_text, return_tensors=\"pt\").to(\n",
    "    \"cuda:0\"\n",
    ")\n",
    "rejected_input_ids = rejected_tokenized_text.input_ids\n",
    "print(f\"rejected_input_ids shape: {rejected_input_ids.shape}\")\n",
    "rejected_attention_mask = rejected_tokenized_text.attention_mask\n",
    "\n",
    "# Forward\n",
    "# (Policy) model\n",
    "# Log probabilities of chosen texts\n",
    "policy_chosen_outputs = model(\n",
    "    input_ids=chosen_input_ids, attention_mask=chosen_attention_mask\n",
    ")\n",
    "policy_chosen_log_probs = F.log_softmax(policy_chosen_outputs.logits, dim=-1)\n",
    "print(f\"policy_chosen_log_probs shape: {policy_chosen_log_probs.shape}\")\n",
    "print(f\"policy_chosen_log_probs:\\n {policy_chosen_log_probs}\")\n",
    "\n",
    "# Log probabilities of rejected texts\n",
    "policy_rejected_outputs = model(\n",
    "    input_ids=rejected_input_ids, attention_mask=rejected_attention_mask\n",
    ")\n",
    "policy_rejected_log_probs = F.log_softmax(policy_rejected_outputs.logits, dim=-1)\n",
    "print(f\"policy_rejected_log_probs shape: {policy_rejected_log_probs.shape}\")\n",
    "print(f\"policy_rejected_log_probs:\\n {policy_rejected_log_probs}\")\n",
    "\n",
    "# Reference model\n",
    "with torch.no_grad():\n",
    "    # Log probabilities of chosen texts\n",
    "    refer_chosen_outputs = ref_model(\n",
    "        input_ids=chosen_input_ids, attention_mask=chosen_attention_mask\n",
    "    )\n",
    "    refer_chosen_log_probs = F.log_softmax(refer_chosen_outputs.logits, dim=-1)\n",
    "    print(f\"refer_chosen_log_probs shape: {refer_chosen_log_probs.shape}\")\n",
    "    print(f\"refer_chosen_log_probs:\\n {refer_chosen_log_probs}\")\n",
    "\n",
    "    # Log probabilities of rejected texts\n",
    "    refer_rejected_outputs = ref_model(\n",
    "        input_ids=rejected_input_ids, attention_mask=rejected_attention_mask\n",
    "    )\n",
    "    refer_rejected_log_probs = F.log_softmax(refer_rejected_outputs.logits, dim=-1)\n",
    "    print(f\"refer_rejected_log_probs shape: {refer_rejected_log_probs.shape}\")\n",
    "    print(f\"refer_rejected_log_probs:\\n {refer_rejected_log_probs}\")\n",
    "\n",
    "# Extract response part only\n",
    "prompt_length = tokenizer(\n",
    "    prompt + tokenizer.eos_token, return_tensors=\"pt\"\n",
    ").input_ids.shape[1]\n",
    "print(f\"prompt_length: {prompt_length}\")\n",
    "policy_chosen_log_probs_response = policy_chosen_log_probs[:, prompt_length - 1 : -1, :]\n",
    "print(f\"policy_chosen_log_probs_response shape: {policy_chosen_log_probs_response.shape}\")\n",
    "refer_chosen_log_probs_response = refer_chosen_log_probs[:, prompt_length - 1 : -1, :]\n",
    "print(f\"refer_chosen_log_probs_response shape: {refer_chosen_log_probs_response.shape}\")\n",
    "policy_rejected_log_probs_response = policy_rejected_log_probs[\n",
    "    :, prompt_length - 1 : -1, :\n",
    "]\n",
    "print(f\"policy_rejected_log_probs_response shape: {policy_rejected_log_probs_response.shape}\")\n",
    "refer_rejected_log_probs_response = refer_rejected_log_probs[\n",
    "    :, prompt_length - 1 : -1, :\n",
    "]\n",
    "print(f\"refer_rejected_log_probs_response shape: {refer_rejected_log_probs_response.shape}\")\n",
    "\n",
    "# Targets (Labels)\n",
    "labels_chosen = chosen_input_ids[:, prompt_length:]\n",
    "labels_rejected = rejected_input_ids[:, prompt_length:]\n",
    "print(f\"labels_chosen shape: {labels_chosen.shape}\")\n",
    "print(f\"labels_rejected shape: {labels_rejected.shape}\")\n",
    "\n",
    "def get_response_log_probs(log_probs, labels):\n",
    "    \"\"\"\n",
    "    Gather targets token's probability from vocab_size dim\n",
    "\n",
    "    Args:\n",
    "        log_probs (tensor[batch_size, seq_length, vocab_size]): The generated logit\n",
    "        labels (tensor[batch_size, seq_length]): The true labels (indies)\n",
    "\n",
    "    Returns:\n",
    "        'tensor([batch_size])': Loss of each sample\n",
    "    \"\"\"\n",
    "    # [batch_size, seq_length] -> [batch_size, seq_length, 1]\n",
    "    print(f\"labels shape: {labels.shape}\")\n",
    "    labels = labels.unsqueeze(-1)\n",
    "    print(f\"After unsqueeze, labels shape: {labels.shape}\")\n",
    "    per_token_logps = torch.gather(log_probs, dim=2, index=labels)\n",
    "    print(f\"per_token_logps shape: {per_token_logps.shape}\")\n",
    "\n",
    "    # [batch_size, seq_length, 1] -> [batch_size, sequence] -> [batch_size]\n",
    "    response_log_probs = per_token_logps.squeeze(-1).sum(dim=1)\n",
    "    print(f\"response_log_probs shape: {response_log_probs.shape}\")\n",
    "    return response_log_probs\n",
    "\n",
    "\n",
    "# Loss\n",
    "pi_chosen_log_prob = get_response_log_probs(\n",
    "    policy_chosen_log_probs_response, labels_chosen\n",
    ")\n",
    "print(f\"pi_chosen_log_prob shape: {pi_chosen_log_prob.shape}\")\n",
    "print(f\"pi_chosen_log_prob: {pi_chosen_log_prob}\")\n",
    "pi_rejected_log_probs = get_response_log_probs(\n",
    "    policy_rejected_log_probs_response, labels_rejected\n",
    ")\n",
    "print(f\"pi_rejected_log_probs shape: {pi_rejected_log_probs.shape}\")\n",
    "print(f\"pi_rejected_log_probs: {pi_rejected_log_probs}\")\n",
    "\n",
    "ref_chosen_log_prob = get_response_log_probs(\n",
    "    refer_chosen_log_probs_response, labels_chosen\n",
    ")\n",
    "print(f\"ref_chosen_log_prob shape: {ref_chosen_log_prob.shape}\")\n",
    "print(f\"ref_chosen_log_prob: {ref_chosen_log_prob}\")\n",
    "\n",
    "ref_rejected_log_prob = get_response_log_probs(\n",
    "    refer_rejected_log_probs_response, labels_rejected\n",
    ")\n",
    "print(f\"ref_rejected_log_prob shape: {ref_rejected_log_prob.shape}\")\n",
    "print(f\"ref_rejected_log_prob: {ref_rejected_log_prob}\")\n",
    "\n",
    "# dpo loss\n",
    "policy_reward_chosen = beta * (pi_chosen_log_prob - ref_chosen_log_prob)\n",
    "policy_reward_rejected = beta * (pi_rejected_log_probs - ref_rejected_log_prob)\n",
    "print(f\"policy_reward_chosen shape: {policy_reward_chosen.shape}\")\n",
    "print(f\"policy_reward_rejected shape: {policy_reward_rejected.shape}\")\n",
    "\n",
    "loss = -F.logsigmoid(policy_reward_chosen - policy_reward_rejected).mean()\n",
    "print(f\"loss shape: {loss.shape}\")\n",
    "print(f\"loss: {loss}\")\n",
    "\n",
    "# Backward and steps\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "for param in model.parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"param shape: {param.shape}\")\n",
    "        print(f\"param: {param}\")\n",
    "        print(f\"param grad: {param.grad}\")\n",
    "        assert False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth-env312",
   "language": "python",
   "name": "unsloth-env312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
